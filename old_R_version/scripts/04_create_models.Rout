
R version 3.4.3 (2017-11-30) -- "Kite-Eating Tree"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

Microsoft R Open 3.4.3
The enhanced R distribution from Microsoft
Microsoft packages Copyright (C) 2017 Microsoft Corporation

Using the Intel MKL for parallel mathematical computing (using 28 cores).

Default CRAN mirror snapshot taken on 2018-01-01.
See: https://mran.microsoft.com/.

> source('util.R')
> 
> package_list <- list('devtools', 'data.table', 'stringr', 'glmnet', 'xgboost', 'Matrix')
> 
> # Load and execute specified libraries
> load_or_install(package_list)
 [1] "xgboost"       "glmnet"        "foreach"       "Matrix"       
 [5] "stringr"       "data.table"    "devtools"      "RevoUtils"    
 [9] "stats"         "graphics"      "grDevices"     "utils"        
[13] "datasets"      "RevoUtilsMath" "methods"       "base"         
> 
> ### Read in command line arguments ###
> arg_len <- 16
> args <- commandArgs(TRUE)
> # print(args)
> if (length(args) != arg_len) {
+     stop(sprintf('Must supply %i arguments -- you supplied %i\n', arg_len, length(args)))
+ } else {
+     splits <- as.character(args[1])
+     # cat(sprintf('splits: %s\n', splits))
+     outcomes <- as.character(args[2])
+     # cat(sprintf('outcomes: %s\n', outcomes))
+     names <- as.character(args[3])
+     # cat(sprintf('names: %s\n', names))
+     model_types <- as.character(args[4])
+     # cat(sprintf('model_types: %s\n', model_types))
+     data_dir <- as.character(args[5])
+     # cat(sprintf('data_dir: %s\n', data_dir))
+     model_dir <- as.character(args[6])
+     # cat(sprintf('model_dir: %s\n', model_dir))
+     unit_id <- as.character(args[7])
+     # cat(sprintf('unit_id: %s\n', unit_id))
+     cluster_id <- as.character(args[8])
+     # cat(sprintf('cluster_id: %s\n', cluster_id))
+     learning_rate <- as.numeric(args[9])
+     # cat(sprintf('learning_rate: %s\n', learning_rate))
+     obj <- as.character(args[10])
+     # cat(sprintf('obj: %s\n', obj))
+     scale_pos_weight <- as.numeric(args[11])
+     # cat(sprintf('scale_pos_weight: %s\n', scale_pos_weight))
+     eval_metric <- as.character(args[12])
+     # cat(sprintf('eval_metric: %s\n', eval_metric))
+     max_depth <- as.numeric(args[13])
+     # cat(sprintf('max_depth: %s\n', max_depth))
+     nround <- as.numeric(args[14])
+     # cat(sprintf('nround: %s\n', nround))
+     colsample_bytree <- as.numeric(args[15])
+     # cat(sprintf('colsample_bytree: %s\n', colsample_bytree))
+     seed <- as.numeric(args[16])
+     # cat(sprintf('seed: %s\n', seed))
+ }
> 
> # convert multi-argument arguments to vectors
> splits <- str_split(splits, '--')[[1]]
> outcomes <- str_split(outcomes, '--')[[1]]
> names <- str_split(names, '--')[[1]]
> model_types <- str_split(model_types, '--')[[1]]
> 
> a <- length(splits)
> b <- length(outcomes)
> c <- length(names)
> d <- length(model_types)
> 
> if (length(unique(c(a,b,c,d))) != 1) {
+     stop('All multi-argument arguments must have same length\n')
+ }
> 
> ### Main function ###
> createModels <- function(splits, outcomes, names, model_types, data_dir, model_dir, unit_id, cluster_id, learning_rate, obj, scale_pos_weight, eval_metric, max_depth, nround, colsample_bytree, seed) {
+     prediction_data_dir <- sprintf('%s03_data_with_predictions/', data_dir)
+     unlink(prediction_data_dir, recursive = TRUE)
+     dir.create(prediction_data_dir)
+ 
+     unlink(model_dir, recursive = TRUE)
+     dir.create(model_dir)
+ 
+     # load training data
+     loadData(outcomes, names, data_dir)
+ 
+     # iterate over training sets and train each model
+     prediction_cols <<- c()
+     for (i in 1:length(names)) {
+         cat(sprintf('\n### make and run models for %s ###\n\n', names[i]))
+         other_outcomes <- outcomes[outcomes != outcomes[i]] # vector of outcomes we don't want -- these will be removed before modeling
+         makeAndRunModels(dt_full = get(sprintf('%s_dt', names[i])), splits = splits, outcome = outcomes[i], name = names[i], model_type = model_types[i], data_dir, model_dir, unit_id, cluster_id, other_outcomes, prediction_cols, learning_rate, obj, scale_pos_weight, eval_metric, max_depth, nround, colsample_bytree, seed)
+     }
+ 
+     # create new holdout_dt and ensemble_train_dt (now including predicted outcomes)
+     cat('\nadd predictions to ensemble_train and holdout\n')
+     saveRDS(holdout_dt, sprintf('%sholdout_data_with_predictions.rds', prediction_data_dir))
+     saveRDS(ensemble_train_dt, sprintf('%sensemble_train_data_with_predictions.rds', prediction_data_dir))
+ }
> 
> ### component functions ###
> loadData <- function(outcomes, names, data_dir) {
+     # read in training data
+     for (i in 1:length(outcomes)) {
+         cat(sprintf('loading train_%s_data.rds as %s_dt\n', names[i], names[i]))
+         assign(sprintf('%s_dt', names[i]), readRDS(sprintf('%s02_modeling_data/train_%s_data.rds', data_dir, names[i])), envir = .GlobalEnv)
+     }
+ 
+     # read in ensemble training and holdout data to which we will add predictions
+     assign('ensemble_train_dt', readRDS(sprintf('%s02_modeling_data/ensemble_train_data.rds', data_dir)), envir=.GlobalEnv)
+     cat(sprintf('loading ensemble_train_data.rds as ensemble_train_dt\n'))
+ 
+     assign('holdout_dt', readRDS(sprintf('%s02_modeling_data/holdout_data.rds', data_dir)), envir=.GlobalEnv)
+     cat(sprintf('loading holdout_data.rds as holdout_dt\n'))
+ }
> 
> makeAndRunModels <- function(dt_full, splits, outcome, name, model_type, data_dir, model_dir, unit_id, cluster_id, other_outcomes, prediction_cols, learning_rate, obj, scale_pos_weight, eval_metric, max_depth, nround, colsample_bytree, seed) {
+     # remove extraneous outcome columns, as well as ID columns
+     extra_cols <- c(other_outcomes, prediction_cols, splits, unit_id, cluster_id)
+     remove_cols <- extra_cols[extra_cols %in% names(dt_full)]
+ 
+     dt <- dt_full[, -remove_cols, with = FALSE]
+     ensemble_train_temp <- ensemble_train_dt[, -remove_cols, with = FALSE]
+     holdout_temp <- holdout_dt[, -remove_cols, with = FALSE]
+ 
+     cat(sprintf('ncol(dt) = %i, ncol(holdout_temp) = %i\n', ncol(dt), ncol(holdout_temp)))
+ 
+     # store features and outcome separately
+     keep_cols <- setdiff(names(dt), c(outcome))
+ 
+     X <- Matrix(data.matrix(dt[, ..keep_cols]), sparse = TRUE) # not sure how to get around converting to data.matrix first
+     Y <- Matrix(dt[, get(outcome)], sparse = TRUE)
+ 
+     # define ensemble_train and holdout feature sets
+     holdout_X <-  Matrix(data.matrix(holdout_temp[, ..keep_cols]), sparse = TRUE)
+     cat(sprintf('ncol(X) = %i, ncol(holdout_X) = %i\n', ncol(X), ncol(holdout_X)))
+     stopifnot(ncol(X) == ncol(holdout_X))
+     ensemble_train_X <-  Matrix(data.matrix(ensemble_train_temp[, ..keep_cols]), sparse = TRUE)
+     stopifnot(ncol(X) == ncol(ensemble_train_X))
+ 
+     # run LASSO if specified to do so
+     if (grepl('lasso', model_type)) {
+         cat('run lasso\n')
+         # run and save lasso
+         # registerDoParallel(min(detectCores() - 1, 10))
+         # lasso <- cv.glmnet(x = X, y = Y, alpha = 1, family = 'binomial', nfolds = 10, parallel = TRUE)[9:10]
+         lasso <- cv.glmnet(x = X, y = Y, alpha = 1, family = 'binomial', nfolds = 10)
+         lambda_1se <- lasso$lambda.1se # 'optimal' lambda
+         saveRDS(lasso, sprintf('%s%s_lasso.rds', model_dir, name))
+ 
+         # use lasso to predict outcome in ensemble_train and holdout
+         cat('predict in holdout set\n')
+         predictions <- predict(lasso, holdout_X, s = lambda_1se, type = 'response')
+         assign('holdout_dt', holdout_dt[, (sprintf('%s_lasso_prediction', name)) := predictions], envir=.GlobalEnv)
+ 
+         cat('predict in ensemble training set\n')
+         predictions <- predict(lasso, ensemble_train_X, s = lambda_1se, type = 'response')
+         assign('ensemble_train_dt', ensemble_train_dt[, (sprintf('%s_lasso_prediction', name)) := predictions], envir=.GlobalEnv)
+ 
+         prediction_cols <<- c(prediction_cols, sprintf('%s_lasso_prediction', name))
+ 
+         rm(lasso)
+     }
+ 
+     # run gradient boosted tree if specified to do so
+     if (grepl('gbt', model_type)) {
+         cat('train gradient boosted tree\n')
+ 
+         gbt <- xgboost(data = X,
+                        label = Y,
+                        max.depth = max_depth,
+                        eta = learning_rate,
+                        scale_pos_weight = scale_pos_weight,
+                        colsample_bytree = colsample_bytree,
+                        nround = nround,
+                        objective = obj,
+                        eval_metric = eval_metric,
+                        seed = seed,
+                        verbose = 1
+                        )
+ 
+         saveRDS(gbt, sprintf('%s%s_gbt.rds', model_dir, name))
+ 
+         # use gbt to predict outcome in ensemble_train and holdout
+         cat('predict in holdout set\n')
+         assign('holdout_dt', holdout_dt[, (sprintf('%s_gbt_prediction', name)) := predict(gbt, holdout_X, type = 'response')], envir=.GlobalEnv)
+ 
+         cat('predict in ensemble training set\n')
+         assign('ensemble_train_dt', ensemble_train_dt[, (sprintf('%s_gbt_prediction', name)) := predict(gbt, ensemble_train_X, type = 'response')], envir=.GlobalEnv)
+ 
+         prediction_cols <<- c(prediction_cols, sprintf('%s_gbt_prediction', name))
+ 
+         rm(gbt)
+     }
+ }
> 
> ### execute ###
> createModels(splits, outcomes, names, model_types, data_dir, model_dir, unit_id, cluster_id, learning_rate, obj, scale_pos_weight, eval_metric, max_depth, nround, colsample_bytree, seed)
loading train_joint_outcome_data.rds as joint_outcome_dt
loading train_untested_mace_data.rds as untested_mace_dt
loading train_tested_int_data.rds as tested_int_dt
loading ensemble_train_data.rds as ensemble_train_dt
loading holdout_data.rds as holdout_dt

### make and run models for joint_outcome ###

ncol(dt) = 4326, ncol(holdout_temp) = 4326
ncol(X) = 4325, ncol(holdout_X) = 4325
train gradient boosted tree
[1]	train-auc:0.821719 
[2]	train-auc:0.863229 
[3]	train-auc:0.865270 
[4]	train-auc:0.867708 
[5]	train-auc:0.873729 
[6]	train-auc:0.875159 
[7]	train-auc:0.874598 
[8]	train-auc:0.876327 
[9]	train-auc:0.888019 
[10]	train-auc:0.888067 
[11]	train-auc:0.889063 
[12]	train-auc:0.889854 
[13]	train-auc:0.889995 
[14]	train-auc:0.890582 
[15]	train-auc:0.890734 
[16]	train-auc:0.890790 
[17]	train-auc:0.891635 
[18]	train-auc:0.893120 
[19]	train-auc:0.893372 
[20]	train-auc:0.893538 
[21]	train-auc:0.893753 
[22]	train-auc:0.893713 
[23]	train-auc:0.893867 
[24]	train-auc:0.893950 
[25]	train-auc:0.894174 
[26]	train-auc:0.894605 
[27]	train-auc:0.894540 
[28]	train-auc:0.894589 
[29]	train-auc:0.895045 
[30]	train-auc:0.895107 
[31]	train-auc:0.895317 
[32]	train-auc:0.895443 
[33]	train-auc:0.895905 
[34]	train-auc:0.896138 
[35]	train-auc:0.896373 
[36]	train-auc:0.896563 
[37]	train-auc:0.896775 
[38]	train-auc:0.896975 
[39]	train-auc:0.898781 
[40]	train-auc:0.899332 
[41]	train-auc:0.899774 
[42]	train-auc:0.899976 
[43]	train-auc:0.900064 
[44]	train-auc:0.900234 
[45]	train-auc:0.900977 
[46]	train-auc:0.904310 
[47]	train-auc:0.904419 
[48]	train-auc:0.904630 
[49]	train-auc:0.904786 
[50]	train-auc:0.905420 
[51]	train-auc:0.905693 
[52]	train-auc:0.905776 
[53]	train-auc:0.906153 
[54]	train-auc:0.909386 
[55]	train-auc:0.910419 
[56]	train-auc:0.910883 
[57]	train-auc:0.911257 
[58]	train-auc:0.911577 
[59]	train-auc:0.911725 
[60]	train-auc:0.912064 
[61]	train-auc:0.912778 
[62]	train-auc:0.913014 
[63]	train-auc:0.913225 
[64]	train-auc:0.913409 
[65]	train-auc:0.913624 
[66]	train-auc:0.913906 
[67]	train-auc:0.914388 
[68]	train-auc:0.914797 
[69]	train-auc:0.915205 
[70]	train-auc:0.915497 
[71]	train-auc:0.915627 
[72]	train-auc:0.916021 
[73]	train-auc:0.916750 
[74]	train-auc:0.917408 
[75]	train-auc:0.917633 
[76]	train-auc:0.917745 
[77]	train-auc:0.918063 
[78]	train-auc:0.918153 
[79]	train-auc:0.918310 
[80]	train-auc:0.918628 
[81]	train-auc:0.918732 
[82]	train-auc:0.919026 
[83]	train-auc:0.919460 
[84]	train-auc:0.919571 
[85]	train-auc:0.919798 
[86]	train-auc:0.920100 
[87]	train-auc:0.920507 
[88]	train-auc:0.920787 
[89]	train-auc:0.921061 
[90]	train-auc:0.921654 
[91]	train-auc:0.921918 
[92]	train-auc:0.922107 
[93]	train-auc:0.922379 
[94]	train-auc:0.922505 
[95]	train-auc:0.922757 
[96]	train-auc:0.922911 
[97]	train-auc:0.923087 
[98]	train-auc:0.923306 
[99]	train-auc:0.923551 
[100]	train-auc:0.923678 
[101]	train-auc:0.923858 
[102]	train-auc:0.924154 
[103]	train-auc:0.924354 
[104]	train-auc:0.924533 
[105]	train-auc:0.924791 
[106]	train-auc:0.925029 
[107]	train-auc:0.925384 
[108]	train-auc:0.925568 
[109]	train-auc:0.925763 
[110]	train-auc:0.925925 
[111]	train-auc:0.926082 
[112]	train-auc:0.926324 
[113]	train-auc:0.926510 
[114]	train-auc:0.926679 
[115]	train-auc:0.926885 
[116]	train-auc:0.927115 
[117]	train-auc:0.927368 
[118]	train-auc:0.927582 
[119]	train-auc:0.927782 
[120]	train-auc:0.927966 
[121]	train-auc:0.928083 
[122]	train-auc:0.928340 
[123]	train-auc:0.928532 
[124]	train-auc:0.928737 
[125]	train-auc:0.928896 
[126]	train-auc:0.929093 
[127]	train-auc:0.929304 
[128]	train-auc:0.929469 
[129]	train-auc:0.929619 
[130]	train-auc:0.929806 
[131]	train-auc:0.929979 
[132]	train-auc:0.930097 
[133]	train-auc:0.930322 
[134]	train-auc:0.930443 
[135]	train-auc:0.930624 
[136]	train-auc:0.930789 
[137]	train-auc:0.930892 
[138]	train-auc:0.931043 
[139]	train-auc:0.931202 
[140]	train-auc:0.931345 
[141]	train-auc:0.931558 
[142]	train-auc:0.931731 
[143]	train-auc:0.931928 
[144]	train-auc:0.932080 
[145]	train-auc:0.932248 
[146]	train-auc:0.932373 
[147]	train-auc:0.932482 
[148]	train-auc:0.932646 
[149]	train-auc:0.932848 
[150]	train-auc:0.932949 
[151]	train-auc:0.933107 
[152]	train-auc:0.933247 
[153]	train-auc:0.933438 
[154]	train-auc:0.933506 
[155]	train-auc:0.933654 
[156]	train-auc:0.933785 
[157]	train-auc:0.933949 
[158]	train-auc:0.934081 
[159]	train-auc:0.934198 
[160]	train-auc:0.934424 
[161]	train-auc:0.934563 
[162]	train-auc:0.934635 
[163]	train-auc:0.934734 
[164]	train-auc:0.934902 
[165]	train-auc:0.935000 
[166]	train-auc:0.935146 
[167]	train-auc:0.935260 
[168]	train-auc:0.935369 
[169]	train-auc:0.935442 
[170]	train-auc:0.935531 
[171]	train-auc:0.935673 
[172]	train-auc:0.935802 
[173]	train-auc:0.935912 
[174]	train-auc:0.936044 
[175]	train-auc:0.936130 
[176]	train-auc:0.936313 
[177]	train-auc:0.936436 
[178]	train-auc:0.936556 
[179]	train-auc:0.936648 
[180]	train-auc:0.936734 
[181]	train-auc:0.936818 
[182]	train-auc:0.936916 
[183]	train-auc:0.936973 
[184]	train-auc:0.937099 
[185]	train-auc:0.937197 
[186]	train-auc:0.937302 
[187]	train-auc:0.937357 
[188]	train-auc:0.937404 
[189]	train-auc:0.937486 
[190]	train-auc:0.937524 
[191]	train-auc:0.937598 
[192]	train-auc:0.937636 
[193]	train-auc:0.937732 
[194]	train-auc:0.937765 
[195]	train-auc:0.937885 
[196]	train-auc:0.937953 
[197]	train-auc:0.938024 
[198]	train-auc:0.938107 
[199]	train-auc:0.938214 
[200]	train-auc:0.938260 
[201]	train-auc:0.938313 
[202]	train-auc:0.938354 
[203]	train-auc:0.938433 
[204]	train-auc:0.938500 
[205]	train-auc:0.938543 
[206]	train-auc:0.938614 
[207]	train-auc:0.938685 
[208]	train-auc:0.938753 
[209]	train-auc:0.938801 
[210]	train-auc:0.938886 
[211]	train-auc:0.938925 
[212]	train-auc:0.939006 
[213]	train-auc:0.939063 
[214]	train-auc:0.939137 
[215]	train-auc:0.939274 
[216]	train-auc:0.939336 
[217]	train-auc:0.939383 
[218]	train-auc:0.939496 
[219]	train-auc:0.939584 
[220]	train-auc:0.939607 
[221]	train-auc:0.939679 
[222]	train-auc:0.939804 
[223]	train-auc:0.939926 
[224]	train-auc:0.939980 
[225]	train-auc:0.940021 
[226]	train-auc:0.940078 
[227]	train-auc:0.940131 
[228]	train-auc:0.940167 
[229]	train-auc:0.940229 
[230]	train-auc:0.940335 
[231]	train-auc:0.940415 
[232]	train-auc:0.940492 
[233]	train-auc:0.940540 
[234]	train-auc:0.940582 
[235]	train-auc:0.940651 
[236]	train-auc:0.940723 
[237]	train-auc:0.940833 
[238]	train-auc:0.940886 
[239]	train-auc:0.940921 
[240]	train-auc:0.941004 
[241]	train-auc:0.941043 
[242]	train-auc:0.941173 
[243]	train-auc:0.941234 
[244]	train-auc:0.941265 
[245]	train-auc:0.941312 
[246]	train-auc:0.941392 
[247]	train-auc:0.941400 
[248]	train-auc:0.941431 
[249]	train-auc:0.941477 
[250]	train-auc:0.941499 
[251]	train-auc:0.941551 
[252]	train-auc:0.941583 
[253]	train-auc:0.941624 
[254]	train-auc:0.941661 
[255]	train-auc:0.941701 
[256]	train-auc:0.941798 
[257]	train-auc:0.941857 
[258]	train-auc:0.941935 
[259]	train-auc:0.942055 
[260]	train-auc:0.942100 
[261]	train-auc:0.942162 
[262]	train-auc:0.942214 
[263]	train-auc:0.942284 
[264]	train-auc:0.942373 
[265]	train-auc:0.942405 
[266]	train-auc:0.942493 
[267]	train-auc:0.942582 
[268]	train-auc:0.942634 
[269]	train-auc:0.942687 
[270]	train-auc:0.942735 
[271]	train-auc:0.942795 
[272]	train-auc:0.942858 
[273]	train-auc:0.942911 
[274]	train-auc:0.942947 
[275]	train-auc:0.942990 
[276]	train-auc:0.943056 
[277]	train-auc:0.943117 
[278]	train-auc:0.943193 
[279]	train-auc:0.943253 
[280]	train-auc:0.943384 
[281]	train-auc:0.943424 
[282]	train-auc:0.943453 
[283]	train-auc:0.943484 
[284]	train-auc:0.943522 
[285]	train-auc:0.943587 
[286]	train-auc:0.943648 
[287]	train-auc:0.943731 
[288]	train-auc:0.943829 
[289]	train-auc:0.943891 
[290]	train-auc:0.943920 
[291]	train-auc:0.943982 
[292]	train-auc:0.943993 
[293]	train-auc:0.944022 
[294]	train-auc:0.944128 
[295]	train-auc:0.944169 
[296]	train-auc:0.944244 
[297]	train-auc:0.944305 
[298]	train-auc:0.944349 
[299]	train-auc:0.944387 
[300]	train-auc:0.944435 
[301]	train-auc:0.944478 
[302]	train-auc:0.944553 
[303]	train-auc:0.944626 
[304]	train-auc:0.944667 
[305]	train-auc:0.944757 
[306]	train-auc:0.944794 
[307]	train-auc:0.944816 
[308]	train-auc:0.944848 
[309]	train-auc:0.944902 
[310]	train-auc:0.944957 
[311]	train-auc:0.944992 
[312]	train-auc:0.945046 
[313]	train-auc:0.945086 
[314]	train-auc:0.945104 
[315]	train-auc:0.945174 
[316]	train-auc:0.945224 
[317]	train-auc:0.945277 
[318]	train-auc:0.945363 
[319]	train-auc:0.945422 
[320]	train-auc:0.945445 
[321]	train-auc:0.945477 
[322]	train-auc:0.945550 
[323]	train-auc:0.945619 
[324]	train-auc:0.945679 
[325]	train-auc:0.945741 
[326]	train-auc:0.945836 
[327]	train-auc:0.945869 
[328]	train-auc:0.945914 
[329]	train-auc:0.945948 
[330]	train-auc:0.945978 
[331]	train-auc:0.946016 
[332]	train-auc:0.946084 
[333]	train-auc:0.946111 
[334]	train-auc:0.946191 
[335]	train-auc:0.946234 
[336]	train-auc:0.946295 
[337]	train-auc:0.946367 
[338]	train-auc:0.946408 
[339]	train-auc:0.946419 
[340]	train-auc:0.946489 
[341]	train-auc:0.946575 
[342]	train-auc:0.946648 
[343]	train-auc:0.946675 
[344]	train-auc:0.946772 
[345]	train-auc:0.946817 
[346]	train-auc:0.946893 
[347]	train-auc:0.946959 
[348]	train-auc:0.946980 
[349]	train-auc:0.947034 
[350]	train-auc:0.947112 
[351]	train-auc:0.947159 
[352]	train-auc:0.947183 
[353]	train-auc:0.947268 
[354]	train-auc:0.947311 
[355]	train-auc:0.947374 
[356]	train-auc:0.947468 
[357]	train-auc:0.947491 
[358]	train-auc:0.947526 
[359]	train-auc:0.947574 
[360]	train-auc:0.947602 
[361]	train-auc:0.947673 
[362]	train-auc:0.947698 
[363]	train-auc:0.947726 
[364]	train-auc:0.947807 
[365]	train-auc:0.947876 
[366]	train-auc:0.947901 
[367]	train-auc:0.947925 
[368]	train-auc:0.947958 
[369]	train-auc:0.948013 
[370]	train-auc:0.948038 
[371]	train-auc:0.948126 
[372]	train-auc:0.948191 
[373]	train-auc:0.948229 
[374]	train-auc:0.948260 
[375]	train-auc:0.948301 
[376]	train-auc:0.948356 
[377]	train-auc:0.948374 
[378]	train-auc:0.948411 
[379]	train-auc:0.948441 
[380]	train-auc:0.948467 
[381]	train-auc:0.948556 
[382]	train-auc:0.948628 
[383]	train-auc:0.948662 
[384]	train-auc:0.948748 
[385]	train-auc:0.948787 
[386]	train-auc:0.948828 
[387]	train-auc:0.948838 
[388]	train-auc:0.948901 
[389]	train-auc:0.948971 
[390]	train-auc:0.949022 
[391]	train-auc:0.949081 
[392]	train-auc:0.949102 
[393]	train-auc:0.949134 
[394]	train-auc:0.949185 
[395]	train-auc:0.949224 
[396]	train-auc:0.949241 
[397]	train-auc:0.949270 
[398]	train-auc:0.949308 
[399]	train-auc:0.949334 
[400]	train-auc:0.949368 
[401]	train-auc:0.949405 
[402]	train-auc:0.949506 
[403]	train-auc:0.949575 
[404]	train-auc:0.949617 
[405]	train-auc:0.949688 
[406]	train-auc:0.949709 
[407]	train-auc:0.949740 
[408]	train-auc:0.949786 
[409]	train-auc:0.949834 
[410]	train-auc:0.949845 
[411]	train-auc:0.949901 
[412]	train-auc:0.949976 
[413]	train-auc:0.949996 
[414]	train-auc:0.950025 
[415]	train-auc:0.950082 
[416]	train-auc:0.950100 
[417]	train-auc:0.950171 
[418]	train-auc:0.950219 
[419]	train-auc:0.950241 
[420]	train-auc:0.950280 
[421]	train-auc:0.950352 
[422]	train-auc:0.950443 
[423]	train-auc:0.950501 
[424]	train-auc:0.950523 
[425]	train-auc:0.950579 
[426]	train-auc:0.950609 
[427]	train-auc:0.950649 
[428]	train-auc:0.950691 
[429]	train-auc:0.950719 
[430]	train-auc:0.950765 
[431]	train-auc:0.950809 
[432]	train-auc:0.950899 
[433]	train-auc:0.950950 
[434]	train-auc:0.950973 
[435]	train-auc:0.950984 
[436]	train-auc:0.951022 
[437]	train-auc:0.951038 
[438]	train-auc:0.951085 
[439]	train-auc:0.951101 
[440]	train-auc:0.951192 
[441]	train-auc:0.951207 
[442]	train-auc:0.951242 
[443]	train-auc:0.951365 
[444]	train-auc:0.951397 
[445]	train-auc:0.951420 
[446]	train-auc:0.951462 
[447]	train-auc:0.951508 
[448]	train-auc:0.951551 
[449]	train-auc:0.951577 
[450]	train-auc:0.951634 
[451]	train-auc:0.951676 
[452]	train-auc:0.951691 
[453]	train-auc:0.951728 
[454]	train-auc:0.951781 
[455]	train-auc:0.951791 
[456]	train-auc:0.951813 
[457]	train-auc:0.951867 
[458]	train-auc:0.951900 
[459]	train-auc:0.951932 
[460]	train-auc:0.951971 
[461]	train-auc:0.951991 
[462]	train-auc:0.952040 
[463]	train-auc:0.952093 
[464]	train-auc:0.952161 
[465]	train-auc:0.952200 
[466]	train-auc:0.952215 
[467]	train-auc:0.952242 
[468]	train-auc:0.952322 
[469]	train-auc:0.952405 
[470]	train-auc:0.952485 
[471]	train-auc:0.952507 
[472]	train-auc:0.952520 
[473]	train-auc:0.952530 
[474]	train-auc:0.952598 
[475]	train-auc:0.952616 
[476]	train-auc:0.952638 
[477]	train-auc:0.952667 
[478]	train-auc:0.952694 
[479]	train-auc:0.952720 
[480]	train-auc:0.952756 
[481]	train-auc:0.952781 
[482]	train-auc:0.952814 
[483]	train-auc:0.952881 
[484]	train-auc:0.952900 
[485]	train-auc:0.952955 
[486]	train-auc:0.952988 
[487]	train-auc:0.953003 
[488]	train-auc:0.953049 
[489]	train-auc:0.953071 
[490]	train-auc:0.953093 
[491]	train-auc:0.953108 
[492]	train-auc:0.953166 
[493]	train-auc:0.953189 
[494]	train-auc:0.953217 
[495]	train-auc:0.953232 
[496]	train-auc:0.953258 
[497]	train-auc:0.953335 
[498]	train-auc:0.953401 
[499]	train-auc:0.953443 
[500]	train-auc:0.953484 
predict in holdout set
predict in ensemble training set

### make and run models for untested_mace ###

ncol(dt) = 4326, ncol(holdout_temp) = 4327
ncol(X) = 4325, ncol(holdout_X) = 4325
run lasso
predict in holdout set
predict in ensemble training set

### make and run models for tested_int ###

ncol(dt) = 4326, ncol(holdout_temp) = 4328
ncol(X) = 4325, ncol(holdout_X) = 4325
run lasso
predict in holdout set
predict in ensemble training set

add predictions to ensemble_train and holdout
Warning messages:
1: from glmnet Fortran code (error code -94); Convergence for 94th lambda value not reached after maxit=100000 iterations; solutions for larger lambdas returned 
2: from glmnet Fortran code (error code -83); Convergence for 83th lambda value not reached after maxit=100000 iterations; solutions for larger lambdas returned 
3: from glmnet Fortran code (error code -91); Convergence for 91th lambda value not reached after maxit=100000 iterations; solutions for larger lambdas returned 
4: from glmnet Fortran code (error code -92); Convergence for 92th lambda value not reached after maxit=100000 iterations; solutions for larger lambdas returned 
> 
> proc.time()
     user    system   elapsed 
13857.977    25.617 10788.315 
