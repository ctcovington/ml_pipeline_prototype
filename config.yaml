feature_info:
    # directory containing raw feature files
    feature_dir: '/data/zolab/edw_cohort_data/bwh_ed_2010_2015/features/2018_05_14/'

    # partition files into those containing counts and those not -- files should be identified in this section by their prefix (the portion of the filename before the first underscore)
    # NOTE: file prefixes should be separated by '--'
    count_files: 'dia--ed_enc--enc--lab--lvs--med--prc'
    non_count_files: 'dem'

    # set missingness thresholds for the different file types (count and non-count) -- features will be dropped if the proportion of observations missing is greater than <x>
    missingness_threshold_count: 0.01
    missingness_threshold_non_count: 0.01

    # filepath to ECG features -- can be left blank if you don't want to use ECG features
    ecg_filepath: '/data/zolab/stressed_ensemble/data/ecg_feats/ecg_int_hats.rds'

    # whether or not you will be using ECG features
    use_ecg_feats: 'False'

cohort_info:
    # filepath for cohort -- should contain (at a minimum) identifying information and outcome information, as well as any variables on which we'd like to split for later analysis or model building
    cohort_filepath: '/data/zolab/general_ml_pipeline/cohort_data/joint_outcome.csv'

    # observation-level ID variable
    unit_id: 'ed_enc_id'

    # ID variable representing level at within which we do not want to assign across train and holdout sets (e.g. 'ptid' if we want to ensure that all observations from a given 'ptid' are in the same set)
    cluster_id: 'ptid'

modeling_info:
    # splits representing the subsets of the data on which we want to run models
    # NOTE: we use 'full' as the split whenever we want to run a model on the full data
    # NOTE: these are combined with the 'values' arguments to define subsets of the full data (e.g. if the second argument of 'splits' is 'untested' and the second argument of 'values' is '1', then the second subset will be observations for which 'untested == 1')
    # NOTE: splits should be separated by '--'
    splits: 'full--untested--tested'

    # values for split to which we want to subset (for this example, we split into 'untested == 1' and 'tested == 1')
    # NOTE: must be numeric
    # NOTE: can give any value when split == 'full' (should still be numeric though)
    # NOTE: values should be separated by '--'
    values: '0--1--1'

    # outcome of interest in each model
    # NOTE: outcomes should be separated by '--'
    outcomes: 'joint_outcome--mace--int'

    # name describing each model (used for file and variable naming -- you can make these up to be whatever you want)
    # NOTE: names should be separated by '--'
    names: 'joint_outcome--untested_mace--tested_int'

    # type of model to be run -- current choices are 'gbt' and 'lasso'
    # NOTE: model types should be separated by '--'
    model_types: 'gbt--lasso--lasso'

    # proportion of 'cluster_id' values we want in training set
    train_prop: 0.77

    # proportion of 'cluster_id' values we want in ensemble training set
    ensemble_train_prop: 0.03

    # outcome of interest being predicted by ensemble model
    ensemble_outcome: 'int'

    # variable identifying subset on which we want to assess ensemble model performance
    # NOTE: this will likely be the largest sample on which the ensemble outcome is defined
    ensemble_performance_split: 'tested'

    # value for ensemble_performance_split
    # NOTE: must be numeric
    ensemble_performance_split_value: '1'

pipeline_directories:
    # directory for saving data generated by the pipeline
    data_dir: '/data/zolab/general_ml_pipeline/data/'

    # directory for saving models generated by the pipeline
    model_dir: '/data/zolab/general_ml_pipeline/models/'

    # directory for saving output generated by the pipeline
    output_dir: '/data/zolab/general_ml_pipeline/output/'

xgboost:
    # step size shrinkage used in update to prevents overfitting
    learning_rate: 0.05

    # objective function
    obj: 'binary:logistic'

    # controls balance of positive and negative weights -- typically use something close to sum(negative instances) / sum(positive instances)
    scale_pos_weight: 0.10

    # evaluation metric for holdout data
    eval_metric: 'auc'

    # maximum depth of a tree
    max_depth: 6

    # number of rounds for boosting
    nround: 500

    # subsample ratio of columns when constructing each tree
    colsample_bytree: 0.75

    # randomization seed
    seed: 1
